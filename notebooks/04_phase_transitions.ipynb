{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 — Phase Transition Detection: Statistical Mechanics of Market Regimes\n",
    "\n",
    "**Objective:** Apply phase transition framework from statistical mechanics to detect market regime shifts through entropy discontinuities, correlation length divergence (critical slowing down), and observable analogues.\n",
    "\n",
    "**Venues:** Binance BTCUSDT Perp, Bybit BTCUSDT Perp\n",
    "\n",
    "**Key question:** Do crypto markets exhibit phase-transition-like behavior during the Jan 30 – Feb 5 crash? Can we detect regime shifts before they fully materialize?\n",
    "\n",
    "**Golden rule:** Every section ends with *\"The trading implication is...\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data import load_processed\n",
    "from src.entropy import rolling_shannon_entropy\n",
    "from src.phase_transitions import (\n",
    "    realised_volatility,\n",
    "    order_flow_imbalance,\n",
    "    susceptibility,\n",
    "    correlation_length,\n",
    "    detect_entropy_discontinuities,\n",
    "    classify_regime,\n",
    ")\n",
    "from src.visualisation import (\n",
    "    set_style,\n",
    "    VENUE_COLOURS,\n",
    "    REGIME_COLOURS,\n",
    "    plot_phase_observables,\n",
    "    plot_correlation_length_timeseries,\n",
    "    plot_entropy_discontinuities,\n",
    "    plot_regime_timeline,\n",
    ")\n",
    "\n",
    "set_style()\n",
    "\n",
    "PROCESSED_DIR = Path(\"../data/processed\")\n",
    "FIGURES_DIR = Path(\"../figures\")\n",
    "FIGURES_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trade data\n",
    "binance = load_processed(PROCESSED_DIR / \"binance_btcusdt_perp.parquet\")\n",
    "bybit = load_processed(PROCESSED_DIR / \"bybit_btcusdt_perp.parquet\")\n",
    "venues = {\"binance\": binance, \"bybit\": bybit}\n",
    "\n",
    "for name, df in venues.items():\n",
    "    print(f\"{name.capitalize()}: {len(df):,} trades | \"\n",
    "          f\"{df['timestamp'].min()} → {df['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference price series (Binance 1-min last price)\n",
    "price_1m = (\n",
    "    binance.set_index(\"timestamp\")[\"price\"]\n",
    "    .resample(\"1min\")\n",
    "    .last()\n",
    "    .dropna()\n",
    ")\n",
    "print(f\"Price series: {len(price_1m)} 1-min bars, \"\n",
    "      f\"${price_1m.min():,.0f} – ${price_1m.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temperature and Order Parameter Analogues (Task 4a)\n",
    "\n",
    "*Why this matters for the project: In statistical mechanics, phase transitions are characterized by observable quantities like temperature, magnetization (order parameter), and susceptibility. Here we map market observables to these concepts: volatility serves as temperature (energy of the system), net order flow imbalance serves as magnetization (degree of order), and variance of imbalance serves as susceptibility (sensitivity to perturbations). These mappings allow us to apply the phase transition framework to financial markets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on Binance (primary venue)\n",
    "venue_name = \"binance\"\n",
    "df = venues[venue_name]\n",
    "\n",
    "print(f\"Computing phase observables for {venue_name.capitalize()}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute temperature (realized volatility) on 1-second price series\n",
    "price_1s = df.set_index(\"timestamp\")[\"price\"].resample(\"1s\").last().dropna()\n",
    "\n",
    "# Window = 300 seconds (5 minutes)\n",
    "temperature = realised_volatility(price_1s, window=300)\n",
    "temperature = temperature.dropna()\n",
    "\n",
    "print(f\"Temperature (volatility):\")\n",
    "print(f\"  Mean: {temperature.mean():.6f}\")\n",
    "print(f\"  Std: {temperature.std():.6f}\")\n",
    "print(f\"  Min: {temperature.min():.6f}\")\n",
    "print(f\"  Max: {temperature.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute order parameter (order flow imbalance) on trade signs\n",
    "signs = df[\"trade_sign\"].values\n",
    "\n",
    "# Window = 300 trades (~2.6 seconds at 115 trades/s)\n",
    "imbalance = order_flow_imbalance(signs, window=300)\n",
    "\n",
    "# Create series with timestamps\n",
    "imbalance_series = pd.Series(imbalance, index=df[\"timestamp\"])\n",
    "imbalance_series = imbalance_series.dropna()\n",
    "\n",
    "print(f\"\\nOrder parameter (imbalance):\")\n",
    "print(f\"  Mean: {imbalance_series.mean():.6f}\")\n",
    "print(f\"  Std: {imbalance_series.std():.6f}\")\n",
    "print(f\"  Min: {imbalance_series.min():.6f}\")\n",
    "print(f\"  Max: {imbalance_series.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute susceptibility (variance of imbalance)\n",
    "# Window = 1000 trades (~8.7 seconds)\n",
    "susc = susceptibility(imbalance, window=1000)\n",
    "susc_series = pd.Series(susc, index=df[\"timestamp\"])\n",
    "susc_series = susc_series.dropna()\n",
    "\n",
    "print(f\"\\nSusceptibility:\")\n",
    "print(f\"  Mean: {susc_series.mean():.6f}\")\n",
    "print(f\"  Std: {susc_series.std():.6f}\")\n",
    "print(f\"  Min: {susc_series.min():.6f}\")\n",
    "print(f\"  Max: {susc_series.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample observables to 1-minute for cleaner visualization\n",
    "temp_1m = temperature.resample(\"1min\").mean().dropna()\n",
    "imb_1m = imbalance_series.resample(\"1min\").mean().dropna()\n",
    "susc_1m = susc_series.resample(\"1min\").mean().dropna()\n",
    "\n",
    "# Align to common index\n",
    "common_idx = temp_1m.index.intersection(imb_1m.index).intersection(susc_1m.index)\n",
    "temp_1m = temp_1m.loc[common_idx]\n",
    "imb_1m = imb_1m.loc[common_idx]\n",
    "susc_1m = susc_1m.loc[common_idx]\n",
    "\n",
    "print(f\"\\n1-minute resampled observables: {len(common_idx)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all observables\n",
    "fig = plot_phase_observables(\n",
    "    timestamps=common_idx,\n",
    "    temperature=temp_1m.values,\n",
    "    order_parameter=imb_1m.values,\n",
    "    susceptibility=susc_1m.values,\n",
    "    prices=price_1m,\n",
    "    title=f\"{venue_name.capitalize()} — Phase Transition Observables\",\n",
    ")\n",
    "fig.savefig(FIGURES_DIR / \"04_temperature_order_parameter.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check:**\n",
    "- Temperature (volatility) should spike during the Jan 30-31 crash when BTC dropped sharply\n",
    "- Order parameter should show strong imbalances (near ±1) during directional moves\n",
    "- Susceptibility should peak during transition periods where the market is most sensitive\n",
    "- Order parameter mean should be near 0 (market balanced overall) but with significant variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The trading implication is:** These thermodynamic analogues provide a unified framework for characterizing market state. High temperature (volatility) with low order parameter (balanced flow) indicates a \"hot\", chaotic regime where aggressive strategies carry high risk. Low temperature with high order parameter magnitude indicates a \"cold\", trending regime where directional strategies can be sustained. Susceptibility peaks signal the market is most responsive to new information — ideal conditions for information-based execution strategies but dangerous for pure market-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Length Analysis (Task 4b)\n",
    "\n",
    "*Why this matters for the project: Near phase transitions in physics, the correlation length (spatial scale over which system components are correlated) diverges — a phenomenon called \"critical slowing down\". In markets, we measure correlation length as the timescale over which return autocorrelation decays. If correlation length increases sharply before regime shifts, it provides an early warning signal that the current market state is becoming unstable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute returns on 1-second price series\n",
    "returns_1s = price_1s.pct_change().dropna().values\n",
    "\n",
    "print(f\"Computing correlation length (this may take several minutes)...\")\n",
    "print(f\"Total return observations: {len(returns_1s):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rolling correlation length\n",
    "# Window = 3600 seconds (1 hour), threshold = 1/e\n",
    "corr_len = correlation_length(\n",
    "    returns_1s,\n",
    "    window=3600,\n",
    "    threshold=1/np.e,\n",
    "    max_lag=100,\n",
    ")\n",
    "\n",
    "# Create time series\n",
    "corr_len_series = pd.Series(corr_len, index=price_1s.pct_change().dropna().index)\n",
    "corr_len_series = corr_len_series.dropna()\n",
    "\n",
    "print(f\"\\nCorrelation length computed: {len(corr_len_series):,} values\")\n",
    "print(f\"  Mean: {corr_len_series.mean():.2f} lags\")\n",
    "print(f\"  Std: {corr_len_series.std():.2f} lags\")\n",
    "print(f\"  Min: {corr_len_series.min():.2f} lags\")\n",
    "print(f\"  Max: {corr_len_series.max():.2f} lags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to 5-minute for visualization\n",
    "corr_len_5m = corr_len_series.resample(\"5min\").mean().dropna()\n",
    "\n",
    "print(f\"5-minute resampled: {len(corr_len_5m)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation length time series\n",
    "fig = plot_correlation_length_timeseries(\n",
    "    timestamps=corr_len_5m.index,\n",
    "    correlation_length=corr_len_5m.values,\n",
    "    prices=price_1m,\n",
    "    title=f\"{venue_name.capitalize()} — Correlation Length Evolution (Critical Slowing Down)\",\n",
    ")\n",
    "fig.savefig(FIGURES_DIR / \"04_correlation_length.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify correlation length spikes (potential critical points)\n",
    "threshold_90 = corr_len_5m.quantile(0.90)\n",
    "spikes = corr_len_5m[corr_len_5m > threshold_90]\n",
    "\n",
    "print(f\"\\nCorrelation length spikes (>90th percentile = {threshold_90:.2f} lags):\")\n",
    "print(f\"  {len(spikes)} windows detected\")\n",
    "print(f\"\\nTop 5 spikes:\")\n",
    "for ts, val in spikes.nlargest(5).items():\n",
    "    price = price_1m.loc[price_1m.index.searchsorted(ts)] if ts in price_1m.index else np.nan\n",
    "    print(f\"  {ts} | corr_len = {val:.2f} lags | price ≈ ${price:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if correlation length increases precede volatility spikes\n",
    "# Compute forward volatility (next 30 minutes)\n",
    "vol_forward = price_1m.pct_change().rolling(30).std().shift(-30)\n",
    "\n",
    "# Align with correlation length\n",
    "aligned = pd.DataFrame({\n",
    "    \"corr_len\": corr_len_5m,\n",
    "    \"vol_forward\": vol_forward.resample(\"5min\").mean(),\n",
    "}).dropna()\n",
    "\n",
    "# Correlation between correlation length and forward volatility\n",
    "corr_coef = aligned[\"corr_len\"].corr(aligned[\"vol_forward\"])\n",
    "print(f\"\\nCorrelation between correlation length and 30-min forward volatility: {corr_coef:.4f}\")\n",
    "\n",
    "# Test: high correlation length → high forward volatility?\n",
    "high_corr_len = aligned[aligned[\"corr_len\"] > threshold_90]\n",
    "normal_corr_len = aligned[aligned[\"corr_len\"] <= threshold_90]\n",
    "\n",
    "print(f\"\\nForward volatility comparison:\")\n",
    "print(f\"  High corr_len (>90th pct): mean vol = {high_corr_len['vol_forward'].mean():.6f}\")\n",
    "print(f\"  Normal corr_len: mean vol = {normal_corr_len['vol_forward'].mean():.6f}\")\n",
    "print(f\"  Ratio: {high_corr_len['vol_forward'].mean() / normal_corr_len['vol_forward'].mean():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check:**\n",
    "- Correlation length should be positive and bounded (between 0 and max_lag)\n",
    "- Mean correlation length should be in the 1-20 lag range for 1-second returns\n",
    "- Spikes should occur near known volatility events (Jan 30-31 crash)\n",
    "- If critical slowing down holds, high correlation length should precede volatility increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The trading implication is:** Correlation length serves as an early warning indicator for regime transitions. When correlation length increases sharply (diverges), it signals that the market is approaching a critical point where a phase transition is likely. An HFT desk monitoring correlation length in real-time could detect these precursor signals and adjust risk accordingly: reduce position sizes and widen execution thresholds during critical periods (high correlation length, high susceptibility), then increase aggression once the new regime stabilizes (correlation length normalizes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entropy Discontinuities (Task 4c)\n",
    "\n",
    "*Why this matters for the project: Phase transitions in physics are classified by their order: first-order transitions show discontinuous jumps in observables (like water freezing), while second-order transitions are continuous but with diverging derivatives (like the Curie point in magnets). By analyzing Shannon entropy from Phase 3 for discontinuities, we can characterize the nature of market regime shifts and correlate them with specific events like liquidation cascades.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shannon entropy from Phase 3 (5-minute windows)\n",
    "# Recompute here for self-containment\n",
    "entropy_binance = rolling_shannon_entropy(binance, window=\"5min\")\n",
    "\n",
    "print(f\"Shannon entropy loaded: {len(entropy_binance)} windows\")\n",
    "print(f\"  Mean H: {entropy_binance['entropy'].mean():.4f}\")\n",
    "print(f\"  Std H: {entropy_binance['entropy'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect entropy discontinuities\n",
    "entropy_vals = entropy_binance[\"normalised_entropy\"].values\n",
    "discontinuities, derivative = detect_entropy_discontinuities(\n",
    "    entropy_vals,\n",
    "    threshold=2.0,  # 2 standard deviations\n",
    ")\n",
    "\n",
    "print(f\"\\nEntropy discontinuities detected: {discontinuities.sum()}\")\n",
    "print(f\"Derivative statistics:\")\n",
    "print(f\"  Mean: {derivative.mean():.6f}\")\n",
    "print(f\"  Std: {derivative[derivative != 0].std():.6f}\")\n",
    "print(f\"  Max |derivative|: {np.abs(derivative).max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot entropy with discontinuities marked\n",
    "fig = plot_entropy_discontinuities(\n",
    "    timestamps=entropy_binance[\"timestamp\"],\n",
    "    entropy=entropy_vals,\n",
    "    derivative=derivative,\n",
    "    discontinuities=discontinuities,\n",
    "    prices=price_1m,\n",
    "    title=f\"{venue_name.capitalize()} — Entropy Discontinuity Detection\",\n",
    ")\n",
    "fig.savefig(FIGURES_DIR / \"04_entropy_discontinuities.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze discontinuity events\n",
    "disc_times = entropy_binance[\"timestamp\"][discontinuities]\n",
    "disc_entropy = entropy_vals[discontinuities]\n",
    "disc_deriv = derivative[discontinuities]\n",
    "\n",
    "print(f\"\\nTop 10 entropy discontinuities:\")\n",
    "disc_df = pd.DataFrame({\n",
    "    \"timestamp\": disc_times.values,\n",
    "    \"entropy\": disc_entropy,\n",
    "    \"derivative\": disc_deriv,\n",
    "    \"abs_deriv\": np.abs(disc_deriv),\n",
    "})\n",
    "disc_df = disc_df.sort_values(\"abs_deriv\", ascending=False)\n",
    "\n",
    "for idx, row in disc_df.head(10).iterrows():\n",
    "    ts = pd.Timestamp(row[\"timestamp\"])\n",
    "    price = price_1m.loc[price_1m.index.searchsorted(ts)] if ts in price_1m.index else np.nan\n",
    "    print(f\"  {ts} | H = {row['entropy']:.4f} | dH/dt = {row['derivative']:+.4f} | price ≈ ${price:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate discontinuities with price moves\n",
    "# For each discontinuity, check if it coincides with a large price move\n",
    "price_changes = price_1m.pct_change(5).abs()  # 5-minute return magnitude\n",
    "\n",
    "disc_price_moves = []\n",
    "for ts in disc_times:\n",
    "    ts = pd.Timestamp(ts)\n",
    "    if ts in price_changes.index:\n",
    "        disc_price_moves.append(price_changes.loc[ts])\n",
    "    else:\n",
    "        # Find nearest\n",
    "        idx = price_changes.index.searchsorted(ts)\n",
    "        if idx < len(price_changes):\n",
    "            disc_price_moves.append(price_changes.iloc[idx])\n",
    "\n",
    "if disc_price_moves:\n",
    "    disc_price_moves = np.array(disc_price_moves)\n",
    "    mean_all = price_changes.mean()\n",
    "    mean_disc = disc_price_moves.mean()\n",
    "    print(f\"\\nPrice move correlation:\")\n",
    "    print(f\"  Mean |return| at discontinuities: {mean_disc:.4%}\")\n",
    "    print(f\"  Mean |return| overall: {mean_all:.4%}\")\n",
    "    print(f\"  Ratio: {mean_disc / mean_all:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check:**\n",
    "- Entropy discontinuities should align with visible price discontinuities (sharp moves)\n",
    "- Derivative should be near zero most of the time, with spikes at discontinuities\n",
    "- The Jan 30-31 crash period should show multiple discontinuities\n",
    "- Discontinuities should be more frequent during high-volatility periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The trading implication is:** Entropy discontinuities mark regime transitions in real-time. Sharp entropy drops (large negative derivative) signal the onset of informed directional trading; sharp entropy increases signal return to randomness/noise. By classifying these as first-order-like (sudden jumps, likely liquidation cascades) versus second-order-like (gradual transitions, likely regime shifts), a desk can tailor its response: first-order transitions require immediate risk reduction, while second-order transitions allow for adaptive repositioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regime Classification (Task 4d)\n",
    "\n",
    "*Why this matters for the project: Combining all Phase 4 observables into a unified regime classification provides a complete phase-transition framework for market state. By classifying each time window as \"hot\" (disordered, high entropy/volatility), \"cold\" (ordered, low entropy/volatility), or \"critical\" (transition zone), we can test whether these physics-inspired labels correspond to intuitively different market conditions and whether regime transitions are predictive of future volatility.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare aligned observables at 5-minute resolution\n",
    "# Resample all to 5min and align\n",
    "entropy_5m = pd.Series(\n",
    "    entropy_binance[\"normalised_entropy\"].values,\n",
    "    index=entropy_binance[\"timestamp\"],\n",
    ")\n",
    "\n",
    "vol_5m = temp_1m.resample(\"5min\").mean()\n",
    "susc_5m = susc_1m.resample(\"5min\").mean()\n",
    "corr_5m = corr_len_5m\n",
    "\n",
    "# Align to common index\n",
    "common = entropy_5m.index.intersection(vol_5m.index).intersection(susc_5m.index).intersection(corr_5m.index)\n",
    "\n",
    "entropy_aligned = entropy_5m.loc[common].values\n",
    "vol_aligned = vol_5m.loc[common].values\n",
    "susc_aligned = susc_5m.loc[common].values\n",
    "corr_aligned = corr_5m.loc[common].values\n",
    "\n",
    "print(f\"Aligned observables: {len(common)} 5-minute windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify regimes\n",
    "regimes = classify_regime(\n",
    "    volatility=vol_aligned,\n",
    "    entropy=entropy_aligned,\n",
    "    corr_length=corr_aligned,\n",
    "    susceptibility=susc_aligned,\n",
    ")\n",
    "\n",
    "regime_series = pd.Series(regimes, index=common)\n",
    "\n",
    "print(f\"\\nRegime classification:\")\n",
    "for regime in [\"hot\", \"cold\", \"critical\", \"transitional\", \"unknown\"]:\n",
    "    count = (regime_series == regime).sum()\n",
    "    pct = count / len(regime_series)\n",
    "    print(f\"  {regime.capitalize():15s}: {count:4d} ({pct:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regime timeline\n",
    "fig = plot_regime_timeline(\n",
    "    regimes=regime_series,\n",
    "    prices=price_1m,\n",
    "    title=f\"{venue_name.capitalize()} — Market Regime Classification\",\n",
    ")\n",
    "fig.savefig(FIGURES_DIR / \"04_regime_classification.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regime statistics\n",
    "regime_stats = []\n",
    "for regime in [\"hot\", \"cold\", \"critical\", \"transitional\"]:\n",
    "    mask = regime_series == regime\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    regime_stats.append({\n",
    "        \"Regime\": regime.capitalize(),\n",
    "        \"Count\": mask.sum(),\n",
    "        \"Mean Entropy\": entropy_aligned[mask.values].mean(),\n",
    "        \"Mean Volatility\": vol_aligned[mask.values].mean(),\n",
    "        \"Mean Corr Length\": corr_aligned[mask.values].mean(),\n",
    "        \"Mean Susceptibility\": susc_aligned[mask.values].mean(),\n",
    "    })\n",
    "\n",
    "regime_stats_df = pd.DataFrame(regime_stats)\n",
    "print(\"\\nRegime statistics:\")\n",
    "print(regime_stats_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regime transition matrix\n",
    "transitions = []\n",
    "for i in range(1, len(regime_series)):\n",
    "    curr = regime_series.iloc[i - 1]\n",
    "    next_regime = regime_series.iloc[i]\n",
    "    transitions.append((curr, next_regime))\n",
    "\n",
    "transition_df = pd.DataFrame(transitions, columns=[\"from\", \"to\"])\n",
    "transition_matrix = pd.crosstab(\n",
    "    transition_df[\"from\"],\n",
    "    transition_df[\"to\"],\n",
    "    normalize=\"index\",\n",
    ")\n",
    "\n",
    "print(\"\\nRegime transition matrix (row = from, col = to):\")\n",
    "print(transition_matrix.to_string(float_format=lambda x: f\"{x:.3f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: are regime changes predictive of forward volatility?\n",
    "# Identify regime transitions (any change)\n",
    "regime_changes = (regime_series != regime_series.shift(1))\n",
    "\n",
    "# Forward volatility (next 30 minutes)\n",
    "fwd_vol_5m = price_1m.pct_change().rolling(30).std().shift(-30).resample(\"5min\").mean()\n",
    "\n",
    "# Align\n",
    "aligned_test = pd.DataFrame({\n",
    "    \"regime_change\": regime_changes,\n",
    "    \"fwd_vol\": fwd_vol_5m,\n",
    "}).dropna()\n",
    "\n",
    "at_transition = aligned_test[aligned_test[\"regime_change\"]]\n",
    "no_transition = aligned_test[~aligned_test[\"regime_change\"]]\n",
    "\n",
    "print(f\"\\nForward volatility after regime transitions:\")\n",
    "print(f\"  At transitions: mean vol = {at_transition['fwd_vol'].mean():.6f}\")\n",
    "print(f\"  No transitions: mean vol = {no_transition['fwd_vol'].mean():.6f}\")\n",
    "print(f\"  Ratio: {at_transition['fwd_vol'].mean() / no_transition['fwd_vol'].mean():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check:**\n",
    "- \"Hot\" regime should have high entropy and high volatility\n",
    "- \"Cold\" regime should have low entropy and low volatility\n",
    "- \"Critical\" regime should have high susceptibility or diverging correlation length\n",
    "- Regime classification should not flip too rapidly (no excessive oscillation)\n",
    "- The Jan 30-31 crash period should show \"critical\" or \"hot\" regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The trading implication is:** The regime classification provides a unified real-time dashboard of market state. When the market enters a \"critical\" regime (correlation length diverging, susceptibility peaking), it signals an impending regime transition. An HFT desk could reduce position sizes or widen execution thresholds during these unstable periods, and increase aggression once the new regime is established (transition to \"hot\" or \"cold\"). The regime transition matrix quantifies persistence and switching probabilities, enabling Markov-chain-based position sizing and execution timing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE TRANSITION ANALYSIS — KEY METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n--- Temperature (Volatility) ---\")\n",
    "print(f\"  Mean: {temp_1m.mean():.6f}\")\n",
    "print(f\"  Max: {temp_1m.max():.6f} (peak volatility)\")\n",
    "\n",
    "print(f\"\\n--- Order Parameter (Imbalance) ---\")\n",
    "print(f\"  Mean: {imb_1m.mean():.6f}\")\n",
    "print(f\"  Range: [{imb_1m.min():.4f}, {imb_1m.max():.4f}]\")\n",
    "\n",
    "print(f\"\\n--- Correlation Length ---\")\n",
    "print(f\"  Mean: {corr_len_5m.mean():.2f} lags\")\n",
    "print(f\"  Max: {corr_len_5m.max():.2f} lags (critical slowing down)\")\n",
    "print(f\"  Correlation with forward vol: {corr_coef:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Entropy Discontinuities ---\")\n",
    "print(f\"  Total detected: {discontinuities.sum()}\")\n",
    "print(f\"  Mean |price move| at discontinuities: {mean_disc:.4%} (vs {mean_all:.4%} overall)\")\n",
    "\n",
    "print(f\"\\n--- Regime Classification ---\")\n",
    "for regime in [\"hot\", \"cold\", \"critical\", \"transitional\"]:\n",
    "    count = (regime_series == regime).sum()\n",
    "    pct = count / len(regime_series) if len(regime_series) > 0 else 0\n",
    "    print(f\"  {regime.capitalize():15s}: {count} ({pct:.1%})\")\n",
    "\n",
    "print(f\"\\nForward vol at regime transitions: {at_transition['fwd_vol'].mean() / no_transition['fwd_vol'].mean():.2f}x higher\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading Implications Summary\n",
    "\n",
    "| Finding | Implication |\n",
    "|---------|-------------|\n",
    "| Temperature (volatility) spikes during crash | High temperature = high-risk, chaotic regime; reduce position sizes |\n",
    "| Order parameter shows strong imbalances during trends | Magnitude of order parameter indicates strength of directional flow |\n",
    "| Susceptibility peaks at transitions | High susceptibility = market most responsive to new information |\n",
    "| Correlation length diverges before volatility increases | Early warning signal for regime shifts; adjust risk proactively |\n",
    "| Entropy discontinuities align with price moves | Sharp entropy changes mark regime transitions in real-time |\n",
    "| Regime classification quantifies market state | Unified dashboard enables state-dependent execution and position sizing |\n",
    "| Regime transitions predict higher forward volatility | Transition detection enables predictive risk management |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Comes Next\n",
    "\n",
    "Phase 4 has established a phase transition framework for detecting regime shifts. The observables (temperature, order parameter, susceptibility, correlation length) and regime classification provide a physics-grounded toolkit for characterizing market state.\n",
    "\n",
    "**Phase 5 (Metastability)** would extend this by identifying quasi-stable price levels where the market lingers before transitioning — analogous to metastable states in physics (like supercooled water). Using free-energy landscape construction and dwell time analysis, we could identify support/resistance levels from first principles and predict breakout probabilities.\n",
    "\n",
    "**Phase 6 (Synthesis)** will tie everything together: entropy flow (Phase 3) + phase transitions (Phase 4) + metastability (Phase 5) into a unified market state dashboard with concrete trading rules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
